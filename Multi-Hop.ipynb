{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:31:52.141753Z",
     "start_time": "2024-03-27T14:31:45.921927Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emerykarambiri/PycharmProjects/BMKG Assignment 3/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "from typing import Any, List, Optional\n",
    "from operator import itemgetter\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import format_document\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_community.document_loaders.base import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Provide your OpenAI API Key\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_classes_query = \"\"\"\n",
    "PREFIX schema: <http://schema.org/>\n",
    "\n",
    "SELECT ?uri ?predicate ?label ?type\n",
    "WHERE {\n",
    "    ?uri a ?type;\n",
    "        ?predicate ?label.\n",
    "    FILTER (\n",
    "        ?predicate = schema:name\n",
    "    )\n",
    "}\"\"\"\n",
    "\n",
    "class OntologyLoader(BaseLoader):\n",
    "    \"\"\"Load an OWL ontology and extract classes and properties as documents.\"\"\"\n",
    "\n",
    "    def __init__(self, ontology_url: str, format: Optional[str] = None):\n",
    "        self.ontology_url = ontology_url\n",
    "        self.format = format\n",
    "        self.graph = Graph(store=\"Oxigraph\")\n",
    "        self.graph.parse(source=self.ontology_url, format=self.format)\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load and return documents (classes and properties) from the OWL ontology.\"\"\"\n",
    "        docs: List[Document] = []\n",
    "        for cls in self.graph.query(extract_classes_query):\n",
    "            docs.append(self._create_document(cls))\n",
    "        return docs\n",
    "\n",
    "    def _create_document(self, result_row: Any) -> Document:\n",
    "        \"\"\"Create a Document object from a query result row.\"\"\"\n",
    "        label = str(result_row.label)\n",
    "        return Document(\n",
    "            page_content=label,\n",
    "            # NOTE: you can include more metadata retrieved by the SPARQL query here\n",
    "            metadata={\n",
    "                \"label\": label,\n",
    "                \"uri\": str(result_row.uri),\n",
    "                \"type\": str(result_row.type),\n",
    "                \"predicate\": str(result_row.predicate),\n",
    "                \"ontology\": self.ontology_url,\n",
    "            },\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:31:55.062782Z",
     "start_time": "2024-03-27T14:31:54.956499Z"
    }
   },
   "id": "2f7848c4edeb02e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "215c9c4c7dd645f887e819286d47287b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flag_embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", max_length=512)\n",
    "loader = OntologyLoader(\"football_data_small.ttl\", format=\"ttl\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    splits,\n",
    "    flag_embeddings,\n",
    "    collection_name=\"ontologies\",\n",
    "    location=\":memory:\",\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 15})\n",
    "llm = OpenAI(temperature=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:34:29.138414Z",
     "start_time": "2024-03-27T14:31:58.889617Z"
    }
   },
   "id": "27313d478b125cdb",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create the memory object that is used to add messages\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "# Add a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "\n",
    "# Prompt to reformulate the question using the chat history\n",
    "reform_template = \"\"\"\n",
    "You are a football analyst. Reformulate the question such that it contains all the contextual information needed. Do not answer anything, only reformulate the question.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "REFORM_QUESTION_PROMPT = PromptTemplate.from_template(reform_template)\n",
    "\n",
    "# Prompt to ask to answer the reformulated question\n",
    "answer_template = \"\"\"\n",
    "Answer the question as a list, as succinctly as you can. \n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(answer_template)\n",
    "\n",
    "# Format how the ontology concepts are passed as context to the LLM\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(\n",
    "    template=\"Concept label: {page_content} | URI: {uri} | Type: {type} | Predicate: {predicate} | Ontology: {ontology}\"\n",
    ")\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    # print(\"Formatted docs:\", doc_strings)\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "# Reformulate the question using chat history\n",
    "reformulated_question = {\n",
    "    \"reformulated_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | REFORM_QUESTION_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Retrieve the documents using the reformulated question\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"reformulated_question\") | retriever,\n",
    "    \"question\": lambda x: print(\" Reformulated question:\", x[\"reformulated_question\"]) or x[\"reformulated_question\"],\n",
    "    # \"question\": lambda x: x[\"reformulated_question\"],\n",
    "}\n",
    "# Construct the inputs for the final prompt using retrieved documents\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# Generate the answer using the retrieved documents and answer prompt\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# Put the chain together\n",
    "final_chain = loaded_memory | reformulated_question | retrieved_documents | answer\n",
    "\n",
    "def stream_chain(final_chain, memory: ConversationBufferMemory, inputs: dict[str, str]) -> dict[str, Any]:\n",
    "    \"\"\"Ask question, stream the answer output, and return the answer with source documents.\"\"\"\n",
    "    output = {\"answer\": \"\"}\n",
    "    for chunk in final_chain.stream(inputs):\n",
    "        # print(chunk, flush=True)\n",
    "        if \"docs\" in chunk:\n",
    "            output[\"docs\"] = [doc.dict() for doc in chunk[\"docs\"]]\n",
    "            print(\" Documents retrieved:\")\n",
    "            for doc in output[\"docs\"]:\n",
    "                print(f\"路 {doc['page_content']} ({doc['metadata']['uri']})\")\n",
    "            # print(json.dumps(output[\"docs\"], indent=2))\n",
    "        if \"answer\" in chunk:\n",
    "            output[\"answer\"] += chunk[\"answer\"]\n",
    "            print(chunk[\"answer\"], end=\"\", flush=True)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:49:56.110234Z",
     "start_time": "2024-03-27T14:49:56.089685Z"
    }
   },
   "id": "ba82fcda6b1ecb87",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reformulated question:  Can you name a Brazilian player who has played for a Premier League team and won a trophy? Which team did they play for when they won the trophy?\n",
      " Documents retrieved:\n",
      "路 Premier League (http://www.wikidata.org/entity/Q206073)\n",
      "路 Premier League (http://www.wikidata.org/entity/Q9448)\n",
      "路 Campeonato Brasileiro (http://www.wikidata.org/entity/Q1199581)\n",
      "路 Campeonato Brasileiro S茅rie A (http://www.wikidata.org/entity/Q206813)\n",
      "路 Brazil national football team (http://www.wikidata.org/entity/Q83459)\n",
      "路 Russian Premier League (http://www.wikidata.org/entity/Q182165)\n",
      "路 S茫o Paulo FC (http://www.wikidata.org/entity/Q38568)\n",
      "路 Santos F.C. (http://www.wikidata.org/entity/Q80955)\n",
      "路 2014 FIFA World Cup Match 1, Brazil v Croatia (http://www.wikidata.org/entity/Q17486314)\n",
      "路 Cristiano Ronaldo (http://www.wikidata.org/entity/Q11571)\n",
      "路 R茅union Premier League (http://www.wikidata.org/entity/Q2261921)\n",
      "路 S.L. Benfica (http://www.wikidata.org/entity/Q131499)\n",
      "路 Brazil Olympic football team (http://www.wikidata.org/entity/Q899189)\n",
      "路 FC Porto (http://www.wikidata.org/entity/Q128446)\n",
      "路 CONMEBOL-UEFA Cup of Champions (http://www.wikidata.org/entity/Q595275)\n",
      "\n",
      "Answer: Cristiano Ronaldo played for Manchester United when they won the Premier League trophy."
     ]
    }
   ],
   "source": [
    "# set_debug(True)   # Uncomment to enable detailed LangChain debugging\n",
    "output = stream_chain(final_chain, memory, {\n",
    "    \"question\": \"Give me one Brazilian player that has played for a Premier League team and also won a trophy? Which team did he play for when he won?\"\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:51:01.901492Z",
     "start_time": "2024-03-27T14:50:59.616790Z"
    }
   },
   "id": "a918a37e8bcf87b9",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n1. Casemiro\\n2. Christian Cueva\\n3. Denis Cheryshev\\n4. Eden Hazard\\n5. Emil Forsberg'"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['answer']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:47:28.811072Z",
     "start_time": "2024-03-27T14:47:28.775122Z"
    }
   },
   "id": "385ff44902242c47",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a37f78b06d3b2eec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
